{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime \n",
    "from google.cloud import bigquery\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import col,udf,array\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "\n",
    "from pyspark.sql.functions import col,udf,array\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-02-14 12:42:07.810869\n"
     ]
    }
   ],
   "source": [
    "import datetime;print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Exporting BQ Tables to Google Storage\n",
    "\n",
    "bigquery_client = bigquery.Client()\n",
    "dataset_ref = bigquery_client.dataset(\"tom\") # DATASET OF BQ\n",
    "table_ref = dataset_ref.table(\"Yocuda_clean_data_Nov15_Nov17_20171214_v01\") # TABLE NAME OF BQ\n",
    "\n",
    "\n",
    "job = bigquery_client.extract_table(table_ref, 'gs://westfield-tom/datalab/Yocuda_clean_data_Nov15_Nov17_20171214_v01_*.csv') # GOOGLE CLOUD BUCKET\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Importing \n",
    "\n",
    "data = (spark.read.format(\"csv\")\n",
    "      .option(\"header\", \"true\")\n",
    "      .load('gs://westfield-tom/datalab/Yocuda_clean_data_Nov15_Nov17_20171214_v01_*.csv'))\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-02-14 12:42:20.636512\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|          identifier|\n",
      "+--------------------+\n",
      "|pumbatum31@gmail.com|\n",
      "|gemma.cleave@gmai...|\n",
      "|aprilbowles@live....|\n",
      "|martin.pallot@pir...|\n",
      "|rachel@hotmail.co.uk|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Overall\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "overall = data.filter((to_date(col(\"timestamp\"))>'2016-11-30') & (to_date(col(\"timestamp\"))<'2017-12-01') & (col(\"retailer_name\")=='Argos')).select(\"identifier\").distinct()\n",
    "overall.show(5)\n",
    "overall.count()\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-02-14 12:43:26.173739\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|          identifier|     brand_diversity|\n",
      "+--------------------+--------------------+\n",
      "|   -.tmh24@gmail.com|                 0.0|\n",
      "|-indiaismial78672...|-0.13494920397216753|\n",
      "|.j.douglas@hotmai...|                 0.0|\n",
      "|    00673@uk.mcd.com|                 0.0|\n",
      "|00blackswan7@yaho...| 0.22891523252096496|\n",
      "+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11224605"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Brand\n",
    "\n",
    "df1 = data.filter((to_date(col(\"timestamp\"))>'2016-11-30') & \\\n",
    "                  (to_date(col(\"timestamp\"))<'2017-12-01') & \\\n",
    "                  (col(\"brandName\").isNotNull()) & \\\n",
    "                  (col(\"identifier\").isNotNull()) & \\\n",
    "                  (col(\"item_total\")>0) & \\\n",
    "                  (col(\"retailer_name\")=='Argos'))\\\n",
    ".groupBy(\"identifier\",\"brandName\").agg(sum(\"item_total\").alias(\"spend\"))\n",
    "\n",
    "\n",
    "df2 = data.filter((to_date(col(\"timestamp\"))>'2016-11-30') & \\\n",
    "                  (to_date(col(\"timestamp\"))<'2017-12-01') & \\\n",
    "                  (col(\"brandName\").isNotNull()) & \\\n",
    "                  (col(\"identifier\").isNotNull()) & \\\n",
    "                  (col(\"item_total\")>0) & \\\n",
    "                  (col(\"retailer_name\")=='Argos'))\\\n",
    ".groupBy(\"identifier\").agg(sum(\"item_total\").alias(\"spend_overall\"))\n",
    "\n",
    "df3 = df1.join(df2,[\"identifier\"],\"left\")\n",
    "\n",
    "brand = df3.withColumn('entropy', -(col('spend')/ col('spend_overall'))*log10(col('spend')/col('spend_overall')))\\\n",
    ".groupBy(col(\"identifier\")).agg(sum(\"entropy\").alias(\"brand_diversity\"))\n",
    "\n",
    "brand.show(5)\n",
    "brand.count()\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-02-14 12:47:28.502457\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+\n",
      "|          identifier| category_diversity|\n",
      "+--------------------+-------------------+\n",
      "|   -.tmh24@gmail.com|                0.0|\n",
      "|-indiaismial78672...|  0.299594933046483|\n",
      "|.j.douglas@hotmai...|                0.0|\n",
      "|    00673@uk.mcd.com| 0.2863175935284513|\n",
      "|00blackswan7@yaho...|0.22891523252096496|\n",
      "+--------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11224605"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# category\n",
    "\n",
    "df1 = data.filter((to_date(col(\"timestamp\"))>'2016-11-30') & \\\n",
    "                  (to_date(col(\"timestamp\"))<'2017-12-01') & \\\n",
    "                  (col(\"cat2\").isNotNull()) & \\\n",
    "                  (col(\"identifier\").isNotNull()) & \\\n",
    "                  (col(\"item_total\")>0) & \\\n",
    "                  (col(\"retailer_name\")=='Argos'))\\\n",
    ".groupBy(\"identifier\",\"cat2\").agg(sum(\"item_total\").alias(\"spend\"))\n",
    "\n",
    "\n",
    "df2 = data.filter((to_date(col(\"timestamp\"))>'2016-11-30') & \\\n",
    "                  (to_date(col(\"timestamp\"))<'2017-12-01') & \\\n",
    "                  (col(\"cat2\").isNotNull()) & \\\n",
    "                  (col(\"identifier\").isNotNull()) & \\\n",
    "                  (col(\"item_total\")>0) & \\\n",
    "                  (col(\"retailer_name\")=='Argos'))\\\n",
    ".groupBy(\"identifier\").agg(sum(\"item_total\").alias(\"spend_overall\"))\n",
    "\n",
    "df3 = df1.join(df2,[\"identifier\"],\"left\")\n",
    "\n",
    "category = df3.withColumn('entropy', -(col('spend')/ col('spend_overall'))*log10(col('spend')/col('spend_overall')))\\\n",
    ".groupBy(col(\"identifier\")).agg(sum(\"entropy\").alias(\"category_diversity\"))\n",
    "\n",
    "category.show(5)\n",
    "category.count()\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-02-14 12:51:27.587366\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|          identifier|          timeseries|\n",
      "+--------------------+--------------------+\n",
      "|terry.c.gooding@g...|   2.812892847053249|\n",
      "|phil@ashburyhomes...|0.001186187727518...|\n",
      "|kennedytomlin@yah...|  0.8130466839528898|\n",
      "|s.downes7@ntlworl...|0.006122003711990414|\n",
      "|daniel_anders1989...|   1.113609151073028|\n",
      "+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12103974"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.registerTempTable(\"data\")\n",
    "\n",
    "xx1 =data.selectExpr(\"identifier\",\"month(timestamp) as month\", \"year(timestamp) as year\", \"item_total\", \"1 as ind\").filter((to_date(col(\"timestamp\"))>'2016-11-30') & \\\n",
    "                  (to_date(col(\"timestamp\"))<'2017-12-01') & \\\n",
    "                  (col(\"identifier\").isNotNull()) & \\\n",
    "                  (col(\"item_total\")>0) & \\\n",
    "                  (col(\"retailer_name\")=='Argos'))\n",
    "\n",
    "xx2 =data.selectExpr(\"timestamp \", \"1 as ind\").filter((to_date(col(\"timestamp\"))>'2016-11-30') & \\\n",
    "                  (to_date(col(\"timestamp\"))<'2017-12-01') & \\\n",
    "                  (col(\"identifier\").isNotNull()) & \\\n",
    "                  (col(\"item_total\")>0) & \\\n",
    "                  (col(\"retailer_name\")=='Argos'))\\\n",
    ".groupBy(col(\"ind\")).agg(max(\"timestamp\").alias(\"timestamp_mx\"))\n",
    "\n",
    "xx3 = xx1.join(xx2,[\"ind\"],\"left\")\n",
    "\n",
    "xx4 = xx3.selectExpr(\"identifier\",\"((month(timestamp_mx) - month)+ (year(timestamp_mx)- year)) as recency\", \"item_total\")\\\n",
    ".groupBy(col(\"identifier\"),col(\"recency\")).agg(sum(\"item_total\").alias(\"spend\"))\n",
    "\n",
    "\n",
    "timeseries = xx4.selectExpr(\"identifier\", \"((exp((-1) * recency)) * log10(spend)) as timeseries\")\\\n",
    ".groupBy(col(\"identifier\")).agg(sum(\"timeseries\").alias(\"timeseries\"))\n",
    "\n",
    "timeseries.show(5)\n",
    "timeseries.count()\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-02-14 13:00:43.276878\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+--------------------+\n",
      "|          identifier|    brand_diversity| category_diversity|          timeseries|\n",
      "+--------------------+-------------------+-------------------+--------------------+\n",
      "|   -.tmh24@gmail.com|                0.0|                0.0|0.002840198153859...|\n",
      "|-indiaismial78672...|                0.0|  0.299594933046483|8.485499381124429E-5|\n",
      "|.j.douglas@hotmai...|                0.0|                0.0|0.021535558112033094|\n",
      "|    00673@uk.mcd.com|                0.0| 0.2863175935284513|2.288245512950547E-4|\n",
      "|00blackswan7@yaho...|0.22891523252096496|0.22891523252096496|1.813540983064614...|\n",
      "+--------------------+-------------------+-------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "2018-02-14 13:16:03.259777\n"
     ]
    }
   ],
   "source": [
    "combined = overall.join(brand,[\"identifier\"],\"left\").join(category,[\"identifier\"],\"left\").join(timeseries,[\"identifier\"],\"left\")\n",
    "\n",
    "combined.show(5)\n",
    "combined.count()\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-02-14 13:30:27.367572\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+--------------------+\n",
      "|          identifier|    brand_diversity| category_diversity|          timeseries|\n",
      "+--------------------+-------------------+-------------------+--------------------+\n",
      "|   -.tmh24@gmail.com|                0.0|                0.0|0.002840198153859...|\n",
      "|-indiaismial78672...|                0.0|  0.299594933046483|8.485499381124429E-5|\n",
      "|.j.douglas@hotmai...|                0.0|                0.0|0.021535558112033094|\n",
      "|    00673@uk.mcd.com|                0.0| 0.2863175935284513|2.288245512950547E-4|\n",
      "|00blackswan7@yaho...|0.22891523252096496|0.22891523252096496|1.813540983064614...|\n",
      "+--------------------+-------------------+-------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "2018-02-14 13:58:57.988588\n"
     ]
    }
   ],
   "source": [
    "# Importing the data and starting the process\n",
    "data_new = combined\n",
    "\n",
    "# Replacing the missing values by 0 \n",
    "# Although it's not necessary as there are no missing values most of the time\n",
    "data_new = data_new.fillna(0)\n",
    "\n",
    "# Creating spark sql tables for faster extraction of the percentiles\n",
    "data_new.createOrReplaceTempView('temporary_intermediate_table')\n",
    "\n",
    "# Storing the column names in a set (ordered list of sorts)\n",
    "col_names = set(data_new.columns) - {\"identifier\"}\n",
    "\n",
    "# Writing queries to calculate the 99th (upper limit) and 1st (lower limit) percentile\n",
    "upper_limit_query = 'SELECT ' + ', '.join(['Percentile({var_name}, 0.99) AS {var_name}'.format(var_name = i) for i in col_names]) + ' from ' \n",
    "lower_limit_query = 'SELECT ' + ', '.join(['Percentile({var_name}, 0.01) AS {var_name}'.format(var_name = i) for i in col_names]) + ' from ' \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Saving the capping limits in two pandas DataFrames \n",
    "upper_limit = spark.sql(upper_limit_query + 'temporary_intermediate_table').toPandas()\n",
    "lower_limit = spark.sql(lower_limit_query + 'temporary_intermediate_table').toPandas()\n",
    "\n",
    "# Deciding which columns need outlier treatment (Dropping those which have 1st and 99th percentile as same)\n",
    "columns_to_treat = {var_name for var_name in col_names if upper_limit.iloc[0][var_name] > lower_limit.iloc[0][var_name]}\n",
    "\n",
    "# Writing a SQL query for outlier treatment\n",
    "outlier_treated_data = spark.sql('SELECT identifier, '+ ', '.join(['IF({var_name} >= {maximum}, {maximum}, IF({var_name} <={minimum}, {minimum}, {var_name})) AS  {var_name}'.format(var_name = var_name, maximum = upper_limit.iloc[0][var_name], minimum = lower_limit.iloc[0][var_name])  for var_name in columns_to_treat]) + ' FROM ' + 'temporary_intermediate_table')\n",
    "\n",
    "outlier_treated_data.show(5)\n",
    "outlier_treated_data.count()\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-02-14 14:07:34.442299\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-02-14 14:18:35.251610\n"
     ]
    }
   ],
   "source": [
    "(outlier_treated_data.coalesce(1).write.option(\"header\", \"true\").csv(\"gs://westfield-tom/datalab/Loyalty_indicator/outlier_treated_data\"))\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-02-14 14:27:44.296033\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-0ed9b0258278>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mfinal\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mnormalization_missing_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutlier_treated_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-0ed9b0258278>\u001b[0m in \u001b[0;36mnormalization_missing_value\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m#   Transforming the variables as per Maximum and Minimum Value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mcols\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mmaxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"max\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mmins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"min\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mmins\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"min\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"max\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mmins\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"min\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mtoPandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1701\u001b[0m         \"\"\"\n\u001b[1;32m   1702\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1703\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1705\u001b[0m     \u001b[0;31m##########################################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    436\u001b[0m         \"\"\"\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m             \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1129\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1131\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1133\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    881\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 883\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    884\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1026\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1028\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1029\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/socket.pyc\u001b[0m in \u001b[0;36mreadline\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    449\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m                     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rbufsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mEINTR\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# NORMALIZATION: TARGET is at 0th INDEX\n",
    "from pyspark.sql.functions import col,udf,array\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "\n",
    "# NORMALIZATION: TARGET is at 0th INDEX\n",
    "from pyspark.sql.functions import col,udf,array\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "\n",
    "\n",
    "# Defining the function to normalize the data.\n",
    "def normalization_missing_value(data):\n",
    "  i = 0\n",
    "#   Transforming the variables as per Maximum and Minimum Value\n",
    "  for cols in data.columns[1:]:\n",
    "    maxs = data.agg(max(col(cols)).alias(\"max\")).toPandas()\n",
    "    mins = data.agg(min(col(cols)).alias(\"min\")).toPandas()\n",
    "    data = data.withColumn(cols,((col(cols)-mins[\"min\"][0])/(maxs[\"max\"][0]-mins[\"min\"][0])))\n",
    "    i = i+1\n",
    "  return data\n",
    "                           \n",
    "final= normalization_missing_value(outlier_treated_data)\n",
    "final.show(5)\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################ Defining Target Variable ############################################\n",
    "\n",
    "# Spend Based Model \n",
    "from pyspark.sql.window import Window\n",
    "df1 = data.filter((to_date(col(\"timestamp\"))>'2016-11-30') & \\\n",
    "                  (to_date(col(\"timestamp\"))<'2017-12-01') & \\\n",
    "                  (col(\"identifier\").isNotNull()) & \\\n",
    "                  (col(\"item_total\")>0))\\\n",
    ".groupBy(\"identifier\").agg(sum(\"item_total\").alias(\"spend\"))\n",
    "\n",
    "df2 = df1.withColumn('percentile', percent_rank().over(Window.orderBy(desc('spend')))) \\\n",
    ".select('identifier','spend', 'percentile')\n",
    "\n",
    "df3 = df2.withColumn(\"bucket\", (when(col(\"percentile\")<=0.10,\"0-10\").otherwise(\\\n",
    "      when(col('percentile')<=0.20,\"10-20\").otherwise(\\\n",
    "        when(col('percentile')<=0.30,\"20-30\").otherwise(\\\n",
    "          when(col('percentile')<=0.40,\"30-40\").otherwise(\\\n",
    "            when(col('percentile')<=0.50,\"40-50\").otherwise(\\\n",
    "              when(col('percentile')<=0.60,\"50-60\").otherwise(\\\n",
    "                when(col('percentile')<=0.70,\"60-70\").otherwise(\\\n",
    "                  when(col('percentile')<=0.80,\"70-80\").otherwise(\\\n",
    "                    when(col('percentile')<=0.90,\"80-90\").otherwise('90-100')))))))))))\\\n",
    "\n",
    "output = df3.groupBy(\"bucket\").agg((sum(col(\"spend\")).alias(\"spend\"))\\\n",
    "                       ,(countDistinct(col(\"identifier\")).alias(\"cnt_shp\"))\\\n",
    "                       ,(avg(col(\"spend\")).alias(\"avg_spend\")))\n",
    "\n",
    "output.show()\n",
    "print(datetime.datetime.now())\n",
    "######################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################ Defining Target Variable ############################################\n",
    "\n",
    "# Spend Based Model \n",
    "from pyspark.sql.window import Window\n",
    "df1 = data.filter((to_date(col(\"timestamp\"))>'2016-11-30') & \\\n",
    "                  (to_date(col(\"timestamp\"))<'2017-12-01') & \\\n",
    "                  (col(\"identifier\").isNotNull()) & \\\n",
    "                  (col(\"item_total\")>0))\\\n",
    ".groupBy(\"identifier\").agg(sum(\"item_total\").alias(\"spend\"))\n",
    "\n",
    "df2 = df1.withColumn('percentile', percent_rank().over(Window.orderBy(desc('spend')))) \\\n",
    ".select('identifier','spend', 'percentile')\n",
    "\n",
    "target = df2.withColumn(\"target\",(when(col(\"percentile\")<=0.20,1).otherwise(0)))\\\n",
    "                     .select('identifier','target')\n",
    "\n",
    "final_w_target = final.join(target,[\"identifier\"],\"left\").fillna(0)\n",
    "\n",
    "final_w_target.show(5)\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "import random\n",
    "random.seed(7)\n",
    "train, test = final_w_target.randomSplit([0.8, 0.2])\n",
    "\n",
    "train.show(5)\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####### Saving Test and train dataset ############\n",
    "\n",
    "train.write.mode(\"overwrite\").parquet(\"gs://westfield-tom/datalab/train_set2\")\n",
    "test.write.mode(\"overwrite\").parquet(\"gs://westfield-tom/datalab/test_set2\")\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################################################################################################################\n",
    "################################################## RUN Data Proc Code ################################################\n",
    "######################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################ To Run After training the model  ##################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Coeffecients and Intercept for Logistic Regression ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import *\n",
    "print ('Reading Saved Model...')\n",
    "\n",
    "lrModel = LogisticRegressionModel.read().load(\"gs://ds-mlengine/ujjwal/loyalty_tier/lr_model1\")\n",
    "\n",
    "# Print the coefficients and intercept for logistic regression\n",
    "print ('Model Coeffecients...')\n",
    "print(\"Coefficients: \" + str(lrModel.coefficients))\n",
    "print(\"Intercept: \" + str(lrModel.intercept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print ('Reading Saved Data...')\n",
    "train_predictions = sqlContext.read.parquet(\"gs://ds-mlengine/ujjwal/loyalty_tier/train_predictions\")\n",
    "print ('Sample Records...')\n",
    "train_predictions.select(\"prediction\", \"features\", \"probability\", \"label\").show(5)\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Confusion matrix\n",
    "#Logic for getting confusion matrix and model evaluation parameters for Cross Validation Data\n",
    "TP = train_predictions.filter((col(\"prediction\") == 1) &  (col(\"label\")==1)).count()\n",
    "FP = train_predictions.filter((col(\"prediction\") == 1) &  (col(\"label\") == 0)).count()\n",
    "TN = train_predictions.filter((col(\"prediction\") == 0) &  (col(\"label\") == 0)).count()\n",
    "FN = train_predictions.filter((col(\"prediction\") == 0) &  (col(\"label\") == 1)).count()\n",
    "\n",
    "print 'True Positive = ', TP\n",
    "print 'False Negative = ', FN\n",
    "print 'True Negative = ', TN\n",
    "print 'False Positive = ', FP\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# KS For Cross Validation\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "\n",
    "outlier_treated_cv=train\n",
    "\n",
    "split1_udf = udf(lambda value: value[0].item(), FloatType())\n",
    "split2_udf = udf(lambda value: value[1].item(), FloatType())\n",
    "prediction_prob = train_predictions.select(split1_udf('probability').alias('Prob_0'), split2_udf('probability').alias('Prob_1'),\"label\")\n",
    "\n",
    "\n",
    "# Size for each bins\n",
    "bin_size = float(outlier_treated_cv.count()/10)\n",
    "# Total Events\n",
    "total_events = float(str(outlier_treated_cv.select(sum(\"target\")).head()).replace(\")\",\"\").split(\"=\")[1])\n",
    "# Total non events\n",
    "total_non_events = outlier_treated_cv.count() - float(str(outlier_treated_cv.select(sum(\"Target\")).head()).replace(\")\",\"\").split(\"=\")[1])\n",
    "\n",
    "# Decile Tables used for KS\n",
    "x = prediction_prob.withColumn(\"row_number\",ceil(F.row_number().over(Window.partitionBy().orderBy(\"prob_1\"))/bin_size)).groupBy(\"row_number\").agg(min(\"prob_1\").alias(\"minimum\"),max(\"prob_1\").alias(\"maximum\"),sum(\"label\").alias(\"number_of_target\"), (sum(\"label\")/int(bin_size)).alias(\"perc_of_target\"),(int(bin_size)-sum(\"label\")).alias(\"number_of_non_target\"))\n",
    "x= x.filter(col(\"row_number\")<=10).withColumn(\"cum_tar\", sum(\"number_of_target\").over(Window.partitionBy().orderBy(\"row_number\"))).withColumn(\"cum_non_tar\", sum(\"number_of_non_target\").over(Window.partitionBy().orderBy(\"row_number\"))).withColumn(\"per_dist_target\",col(\"cum_tar\")/total_events).withColumn(\"per_dist_non_target\",col(\"cum_non_tar\")/total_non_events).withColumn(\"spread\",abs(col(\"per_dist_target\")-col(\"per_dist_non_target\")))\n",
    "x.show()\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
