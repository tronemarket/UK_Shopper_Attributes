{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime \n",
    "from google.cloud import bigquery\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import col,udf,array\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "\n",
    "from pyspark.sql.functions import col,udf,array\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-02-15 09:36:19.977787\n"
     ]
    }
   ],
   "source": [
    "import datetime;print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-02-15 09:36:20.670759\n"
     ]
    }
   ],
   "source": [
    "# Exporting BQ Tables to Google Storage\n",
    "\n",
    "bigquery_client = bigquery.Client()\n",
    "dataset_ref = bigquery_client.dataset(\"tom\") # DATASET OF BQ\n",
    "table_ref = dataset_ref.table(\"Yocuda_clean_data_Nov15_Nov17_20171214_v01\") # TABLE NAME OF BQ\n",
    "\n",
    "\n",
    "job = bigquery_client.extract_table(table_ref, 'gs://westfield-tom/datalab/Yocuda_clean_data_Nov15_Nov17_20171214_v01_*.csv') # GOOGLE CLOUD BUCKET\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-02-15 09:36:20.676615\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-02-15 09:36:53.112431\n"
     ]
    }
   ],
   "source": [
    "#Importing \n",
    "\n",
    "data = (spark.read.format(\"csv\")\n",
    "      .option(\"header\", \"true\")\n",
    "      .load('gs://westfield-tom/datalab/Yocuda_clean_data_Nov15_Nov17_20171214_v01_*.csv'))\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-02-15 09:36:53.117959\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|          identifier|\n",
      "+--------------------+\n",
      "|sazidchoudhury@gm...|\n",
      "|shakur_b9@hotmail...|\n",
      "|michaelharteis@ho...|\n",
      "|  rebekah869@msn.com|\n",
      "|     skorp@gmail.com|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "2018-02-15 09:39:02.225298\n"
     ]
    }
   ],
   "source": [
    "# Overall\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "overall = data.filter((to_date(col(\"timestamp\"))>'2016-11-30') & (to_date(col(\"timestamp\"))<'2017-12-01') & (col(\"retailer_name\")=='Argos')).select(\"identifier\").distinct()\n",
    "overall.show(5)\n",
    "overall.count()\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-02-15 09:39:02.231004\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+\n",
      "|          identifier|    brand_diversity|\n",
      "+--------------------+-------------------+\n",
      "|   -.tmh24@gmail.com|                0.0|\n",
      "|-indiaismial78672...|                0.0|\n",
      "|.j.douglas@hotmai...|                0.0|\n",
      "|    00673@uk.mcd.com|                0.0|\n",
      "|00blackswan7@yaho...|0.22891523252096496|\n",
      "+--------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "2018-02-15 09:42:49.232262\n"
     ]
    }
   ],
   "source": [
    "# Brand\n",
    "\n",
    "df1 = data.filter((to_date(col(\"timestamp\"))>'2016-11-30') & \\\n",
    "                  (to_date(col(\"timestamp\"))<'2017-12-01') & \\\n",
    "                  (col(\"brandName\").isNotNull()) & \\\n",
    "                  (col(\"identifier\").isNotNull()) & \\\n",
    "                  (col(\"item_total\")>0) & \\\n",
    "                  (col(\"retailer_name\")=='Argos'))\\\n",
    ".groupBy(\"identifier\",\"brandName\").agg(sum(\"item_total\").alias(\"spend\"))\n",
    "\n",
    "\n",
    "df2 = data.filter((to_date(col(\"timestamp\"))>'2016-11-30') & \\\n",
    "                  (to_date(col(\"timestamp\"))<'2017-12-01') & \\\n",
    "                  (col(\"brandName\").isNotNull()) & \\\n",
    "                  (col(\"identifier\").isNotNull()) & \\\n",
    "                  (col(\"item_total\")>0) & \\\n",
    "                  (col(\"retailer_name\")=='Argos'))\\\n",
    ".groupBy(\"identifier\").agg(sum(\"item_total\").alias(\"spend_overall\"))\n",
    "\n",
    "df3 = df1.join(df2,[\"identifier\"],\"left\")\n",
    "\n",
    "brand = df3.withColumn('entropy', -(col('spend')/ col('spend_overall'))*log10(col('spend')/col('spend_overall')))\\\n",
    ".groupBy(col(\"identifier\")).agg(sum(\"entropy\").alias(\"brand_diversity\"))\n",
    "\n",
    "brand.show(5)\n",
    "brand.count()\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-02-15 09:42:49.238452\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+\n",
      "|          identifier| category_diversity|\n",
      "+--------------------+-------------------+\n",
      "|   -.tmh24@gmail.com|                0.0|\n",
      "|-indiaismial78672...|  0.299594933046483|\n",
      "|.j.douglas@hotmai...|                0.0|\n",
      "|    00673@uk.mcd.com| 0.2863175935284513|\n",
      "|00blackswan7@yaho...|0.22891523252096496|\n",
      "+--------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "2018-02-15 09:46:29.882627\n"
     ]
    }
   ],
   "source": [
    "# category\n",
    "\n",
    "df1 = data.filter((to_date(col(\"timestamp\"))>'2016-11-30') & \\\n",
    "                  (to_date(col(\"timestamp\"))<'2017-12-01') & \\\n",
    "                  (col(\"cat2\").isNotNull()) & \\\n",
    "                  (col(\"identifier\").isNotNull()) & \\\n",
    "                  (col(\"item_total\")>0) & \\\n",
    "                  (col(\"retailer_name\")=='Argos'))\\\n",
    ".groupBy(\"identifier\",\"cat2\").agg(sum(\"item_total\").alias(\"spend\"))\n",
    "\n",
    "\n",
    "df2 = data.filter((to_date(col(\"timestamp\"))>'2016-11-30') & \\\n",
    "                  (to_date(col(\"timestamp\"))<'2017-12-01') & \\\n",
    "                  (col(\"cat2\").isNotNull()) & \\\n",
    "                  (col(\"identifier\").isNotNull()) & \\\n",
    "                  (col(\"item_total\")>0) & \\\n",
    "                  (col(\"retailer_name\")=='Argos'))\\\n",
    ".groupBy(\"identifier\").agg(sum(\"item_total\").alias(\"spend_overall\"))\n",
    "\n",
    "df3 = df1.join(df2,[\"identifier\"],\"left\")\n",
    "\n",
    "category = df3.withColumn('entropy', -(col('spend')/ col('spend_overall'))*log10(col('spend')/col('spend_overall')))\\\n",
    ".groupBy(col(\"identifier\")).agg(sum(\"entropy\").alias(\"category_diversity\"))\n",
    "\n",
    "category.show(5)\n",
    "category.count()\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-02-15 09:46:29.889011\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+\n",
      "|          identifier|         timeseries|\n",
      "+--------------------+-------------------+\n",
      "|andycooksupply@ho...|0.05852528701915769|\n",
      "|judyfashions5@gma...| 1.7594520520367305|\n",
      "|michellearnold600...|0.22629717562804758|\n",
      "|pspearpoint@yahoo...| 0.6085991155504794|\n",
      "|typeitright@bluey...| 2.5440432268136592|\n",
      "+--------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "2018-02-15 09:53:17.592476\n"
     ]
    }
   ],
   "source": [
    "data.registerTempTable(\"data\")\n",
    "\n",
    "xx1 =data.selectExpr(\"identifier\",\"month(timestamp) as month\", \"year(timestamp) as year\", \"item_total\", \"1 as ind\").filter((to_date(col(\"timestamp\"))>'2016-11-30') & \\\n",
    "                  (to_date(col(\"timestamp\"))<'2017-12-01') & \\\n",
    "                  (col(\"identifier\").isNotNull()) & \\\n",
    "                  (col(\"item_total\")>0) & \\\n",
    "                  (col(\"retailer_name\")=='Argos'))\n",
    "\n",
    "xx2 =data.selectExpr(\"timestamp \", \"1 as ind\").filter((to_date(col(\"timestamp\"))>'2016-11-30') & \\\n",
    "                  (to_date(col(\"timestamp\"))<'2017-12-01') & \\\n",
    "                  (col(\"identifier\").isNotNull()) & \\\n",
    "                  (col(\"item_total\")>0) & \\\n",
    "                  (col(\"retailer_name\")=='Argos'))\\\n",
    ".groupBy(col(\"ind\")).agg(max(\"timestamp\").alias(\"timestamp_mx\"))\n",
    "\n",
    "xx3 = xx1.join(xx2,[\"ind\"],\"left\")\n",
    "\n",
    "xx4 = xx3.selectExpr(\"identifier\",\"((month(timestamp_mx) - month)+ (year(timestamp_mx)- year)) as recency\", \"item_total\")\\\n",
    ".groupBy(col(\"identifier\"),col(\"recency\")).agg(sum(\"item_total\").alias(\"spend\"))\n",
    "\n",
    "\n",
    "timeseries = xx4.selectExpr(\"identifier\", \"((exp((-1) * recency)) * log10(spend)) as timeseries\")\\\n",
    ".groupBy(col(\"identifier\")).agg(sum(\"timeseries\").alias(\"timeseries\"))\n",
    "\n",
    "timeseries.show(5)\n",
    "timeseries.count()\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-02-15 09:53:17.599171\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+--------------------+\n",
      "|          identifier|    brand_diversity| category_diversity|          timeseries|\n",
      "+--------------------+-------------------+-------------------+--------------------+\n",
      "|   -.tmh24@gmail.com|                0.0|                0.0|0.002840198153859...|\n",
      "|-indiaismial78672...|                0.0|  0.299594933046483|8.485499381124429E-5|\n",
      "|.j.douglas@hotmai...|                0.0|                0.0|0.021535558112033094|\n",
      "|    00673@uk.mcd.com|                0.0| 0.2863175935284513|2.288245512950547E-4|\n",
      "|00blackswan7@yaho...|0.22891523252096496|0.22891523252096496|1.813540983064614...|\n",
      "+--------------------+-------------------+-------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "2018-02-15 10:06:45.716609\n"
     ]
    }
   ],
   "source": [
    "combined = overall.join(brand,[\"identifier\"],\"left\").join(category,[\"identifier\"],\"left\").join(timeseries,[\"identifier\"],\"left\")\n",
    "\n",
    "combined.show(5)\n",
    "combined.count()\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-02-15 10:06:45.722780\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+--------------------+\n",
      "|          identifier|    brand_diversity| category_diversity|          timeseries|\n",
      "+--------------------+-------------------+-------------------+--------------------+\n",
      "|   -.tmh24@gmail.com|                0.0|                0.0|0.002840198153859...|\n",
      "|-indiaismial78672...|                0.0|  0.299594933046483|8.485499381124429E-5|\n",
      "|.j.douglas@hotmai...|                0.0|                0.0|0.021535558112033094|\n",
      "|    00673@uk.mcd.com|                0.0| 0.2863175935284513|2.288245512950547E-4|\n",
      "|00blackswan7@yaho...|0.22891523252096496|0.22891523252096496|1.813540983064614...|\n",
      "+--------------------+-------------------+-------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "2018-02-15 10:34:35.313893\n"
     ]
    }
   ],
   "source": [
    "# Importing the data and starting the process\n",
    "data_new = combined\n",
    "\n",
    "# Replacing the missing values by 0 \n",
    "# Although it's not necessary as there are no missing values most of the time\n",
    "data_new = data_new.fillna(0)\n",
    "\n",
    "# Creating spark sql tables for faster extraction of the percentiles\n",
    "data_new.createOrReplaceTempView('temporary_intermediate_table')\n",
    "\n",
    "# Storing the column names in a set (ordered list of sorts)\n",
    "col_names = set(data_new.columns) - {\"identifier\"}\n",
    "\n",
    "# Writing queries to calculate the 99th (upper limit) and 1st (lower limit) percentile\n",
    "upper_limit_query = 'SELECT ' + ', '.join(['Percentile({var_name}, 0.99) AS {var_name}'.format(var_name = i) for i in col_names]) + ' from ' \n",
    "lower_limit_query = 'SELECT ' + ', '.join(['Percentile({var_name}, 0.01) AS {var_name}'.format(var_name = i) for i in col_names]) + ' from ' \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Saving the capping limits in two pandas DataFrames \n",
    "upper_limit = spark.sql(upper_limit_query + 'temporary_intermediate_table').toPandas()\n",
    "lower_limit = spark.sql(lower_limit_query + 'temporary_intermediate_table').toPandas()\n",
    "\n",
    "# Deciding which columns need outlier treatment (Dropping those which have 1st and 99th percentile as same)\n",
    "columns_to_treat = {var_name for var_name in col_names if upper_limit.iloc[0][var_name] > lower_limit.iloc[0][var_name]}\n",
    "\n",
    "# Writing a SQL query for outlier treatment\n",
    "outlier_treated_data = spark.sql('SELECT identifier, '+ ', '.join(['IF({var_name} >= {maximum}, {maximum}, IF({var_name} <={minimum}, {minimum}, {var_name})) AS  {var_name}'.format(var_name = var_name, maximum = upper_limit.iloc[0][var_name], minimum = lower_limit.iloc[0][var_name])  for var_name in columns_to_treat]) + ' FROM ' + 'temporary_intermediate_table')\n",
    "\n",
    "outlier_treated_data.show(5)\n",
    "outlier_treated_data.count()\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-02-15 10:42:04.141213\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-02-15 10:57:10.178770\n"
     ]
    }
   ],
   "source": [
    "(outlier_treated_data.coalesce(1).write.option(\"header\", \"true\").csv(\"gs://westfield-tom/datalab/Loyalty_indicator/outlier_treated_data\"))\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-02-15 10:57:10.184738\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+--------------------+\n",
      "|          identifier|    brand_diversity| category_diversity|          timeseries|\n",
      "+--------------------+-------------------+-------------------+--------------------+\n",
      "|   -.tmh24@gmail.com|                0.0|                0.0|9.893906597435384E-4|\n",
      "|-indiaismial78672...|                0.0|0.34327102441366086|2.955946513638719E-5|\n",
      "|.j.douglas@hotmai...|                0.0|                0.0|0.007501969543729179|\n",
      "|    00673@uk.mcd.com|                0.0| 0.3280580637287231|7.971164739463231E-5|\n",
      "|00blackswan7@yaho...|0.34604067677403916| 0.2622873677211728|6.317518752232143E-5|\n",
      "+--------------------+-------------------+-------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "2018-02-15 11:42:54.099715\n"
     ]
    }
   ],
   "source": [
    "# NORMALIZATION: TARGET is at 0th INDEX\n",
    "from pyspark.sql.functions import col,udf,array\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "\n",
    "# NORMALIZATION: TARGET is at 0th INDEX\n",
    "from pyspark.sql.functions import col,udf,array\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "\n",
    "\n",
    "# Defining the function to normalize the data.\n",
    "def normalization_missing_value(data_new):\n",
    "  i = 0\n",
    "#   Transforming the variables as per Maximum and Minimum Value\n",
    "  for cols in data_new.columns[1:]:\n",
    "    maxs = data_new.agg(max(col(cols)).alias(\"max\")).toPandas()\n",
    "    mins = data_new.agg(min(col(cols)).alias(\"min\")).toPandas()\n",
    "    data_new = data_new.withColumn(cols,((col(cols)-mins[\"min\"][0])/(maxs[\"max\"][0]-mins[\"min\"][0])))\n",
    "    i = i+1\n",
    "  return data_new\n",
    "                           \n",
    "final= normalization_missing_value(outlier_treated_data)\n",
    "final.show(5)\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-02-15 11:42:54.105759\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+-------+------------------+\n",
      "|bucket|               spend|cnt_shp|         avg_spend|\n",
      "+------+--------------------+-------+------------------+\n",
      "|  0-10|1.1849354187978327E9|2106163| 562.6038529771118|\n",
      "| 10-20|4.2566265366426456E8|2131922|199.66145743806038|\n",
      "| 20-30|2.5298825609055772E8|2081822|121.52252022053649|\n",
      "| 30-40| 1.754814611097583E8|2122119| 82.69162149236602|\n",
      "| 40-50|1.2177078294859919E8|2095970| 58.09757913930027|\n",
      "| 50-60| 8.995130797959337E7|2099623|42.841647276484096|\n",
      "| 60-70| 6.702846659015419E7|2104122| 31.85578906078364|\n",
      "| 70-80| 5.454329904064297E7|2382295|22.895274951524883|\n",
      "| 80-90|3.0570063189468637E7|1833997|16.668545907909685|\n",
      "|90-100|2.1033439779639587E7|2099218|10.019654833199594|\n",
      "+------+--------------------+-------+------------------+\n",
      "\n",
      "2018-02-15 11:45:15.832439\n"
     ]
    }
   ],
   "source": [
    "################################################ Defining Target Variable ############################################\n",
    "\n",
    "# Spend Based Model \n",
    "from pyspark.sql.window import Window\n",
    "df1 = data.filter((to_date(col(\"timestamp\"))>'2016-11-30') & \\\n",
    "                  (to_date(col(\"timestamp\"))<'2017-12-01') & \\\n",
    "                  (col(\"identifier\").isNotNull()) & \\\n",
    "                  (col(\"item_total\")>0))\\\n",
    ".groupBy(\"identifier\").agg(sum(\"item_total\").alias(\"spend\"))\n",
    "\n",
    "df2 = df1.withColumn('percentile', percent_rank().over(Window.orderBy(desc('spend')))) \\\n",
    ".select('identifier','spend', 'percentile')\n",
    "\n",
    "df3 = df2.withColumn(\"bucket\", (when(col(\"percentile\")<=0.10,\"0-10\").otherwise(\\\n",
    "      when(col('percentile')<=0.20,\"10-20\").otherwise(\\\n",
    "        when(col('percentile')<=0.30,\"20-30\").otherwise(\\\n",
    "          when(col('percentile')<=0.40,\"30-40\").otherwise(\\\n",
    "            when(col('percentile')<=0.50,\"40-50\").otherwise(\\\n",
    "              when(col('percentile')<=0.60,\"50-60\").otherwise(\\\n",
    "                when(col('percentile')<=0.70,\"60-70\").otherwise(\\\n",
    "                  when(col('percentile')<=0.80,\"70-80\").otherwise(\\\n",
    "                    when(col('percentile')<=0.90,\"80-90\").otherwise('90-100')))))))))))\\\n",
    "\n",
    "output = df3.groupBy(\"bucket\").agg((sum(col(\"spend\")).alias(\"spend\"))\\\n",
    "                       ,(countDistinct(col(\"identifier\")).alias(\"cnt_shp\"))\\\n",
    "                       ,(avg(col(\"spend\")).alias(\"avg_spend\")))\n",
    "\n",
    "output.show()\n",
    "print(datetime.datetime.now())\n",
    "######################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-02-15 11:45:15.838643\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+--------------------+------+\n",
      "|          identifier|    brand_diversity| category_diversity|          timeseries|target|\n",
      "+--------------------+-------------------+-------------------+--------------------+------+\n",
      "|   -.tmh24@gmail.com|                0.0|                0.0|9.893906597435384E-4|     0|\n",
      "|-indiaismial78672...|                0.0|0.34327102441366086|2.955946513638719E-5|     0|\n",
      "|.j.douglas@hotmai...|                0.0|                0.0|0.007501969543729179|     0|\n",
      "|    00673@uk.mcd.com|                0.0| 0.3280580637287231|7.971164739463231E-5|     0|\n",
      "|00blackswan7@yaho...|0.34604067677403916| 0.2622873677211728|6.317518752232143E-5|     0|\n",
      "+--------------------+-------------------+-------------------+--------------------+------+\n",
      "only showing top 5 rows\n",
      "\n",
      "2018-02-15 11:52:45.428069\n"
     ]
    }
   ],
   "source": [
    "################################################ Defining Target Variable ############################################\n",
    "\n",
    "# Spend Based Model \n",
    "from pyspark.sql.window import Window\n",
    "df1 = data.filter((to_date(col(\"timestamp\"))>'2016-11-30') & \\\n",
    "                  (to_date(col(\"timestamp\"))<'2017-12-01') & \\\n",
    "                  (col(\"identifier\").isNotNull()) & \\\n",
    "                  (col(\"item_total\")>0))\\\n",
    ".groupBy(\"identifier\").agg(sum(\"item_total\").alias(\"spend\"))\n",
    "\n",
    "df2 = df1.withColumn('percentile', percent_rank().over(Window.orderBy(desc('spend')))) \\\n",
    ".select('identifier','spend', 'percentile')\n",
    "\n",
    "target = df2.withColumn(\"target\",(when(col(\"percentile\")<=0.20,1).otherwise(0)))\\\n",
    "                     .select('identifier','target')\n",
    "\n",
    "final_w_target = final.join(target,[\"identifier\"],\"left\").fillna(0)\n",
    "\n",
    "final_w_target.show(5)\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-02-15 11:52:45.496895\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+------------------+--------------------+------+\n",
      "|          identifier|    brand_diversity|category_diversity|          timeseries|target|\n",
      "+--------------------+-------------------+------------------+--------------------+------+\n",
      "|.j.douglas@hotmai...|                0.0|               0.0|0.007501969543729179|     0|\n",
      "|    00673@uk.mcd.com|                0.0|0.3280580637287231|7.971164739463231E-5|     0|\n",
      "|00blackswan7@yaho...|0.34604067677403916|0.2622873677211728|6.317518752232143E-5|     0|\n",
      "| 01388dave@gmail.com|                0.0|               0.0|  0.7242505480831564|     0|\n",
      "|01656858872@talkt...| 0.8913519382514233|0.8211887484951836|  0.7681917813342942|     1|\n",
      "+--------------------+-------------------+------------------+--------------------+------+\n",
      "only showing top 5 rows\n",
      "\n",
      "2018-02-15 12:00:22.758377\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "import random\n",
    "random.seed(7)\n",
    "train, test = final_w_target.randomSplit([0.8, 0.2])\n",
    "\n",
    "train.show(5)\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-02-15 12:00:22.764122\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-02-15 12:16:37.918147\n"
     ]
    }
   ],
   "source": [
    "####### Saving Test and train dataset ############\n",
    "\n",
    "train.write.mode(\"overwrite\").parquet(\"gs://westfield-tom/datalab/train_set2\")\n",
    "test.write.mode(\"overwrite\").parquet(\"gs://westfield-tom/datalab/test_set2\")\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################################################################################################################\n",
    "################################################## RUN Data Proc Code ################################################\n",
    "######################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################ To Run After training the model  ##################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Coeffecients and Intercept for Logistic Regression ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-02-15 13:40:00.913915\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Saved Model...\n",
      "Model Coeffecients...\n",
      "Coefficients: [0.9813838992695821,1.531667952505228,1.5192042104267771]\n",
      "Intercept: -1.97776262668\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import *\n",
    "print ('Reading Saved Model...')\n",
    "\n",
    "lrModel = LogisticRegressionModel.read().load(\"gs://westfield-tom/datalab/lr_model1\")\n",
    "\n",
    "# Print the coefficients and intercept for logistic regression\n",
    "print ('Model Coeffecients...')\n",
    "print(\"Coefficients: \" + str(lrModel.coefficients))\n",
    "print(\"Intercept: \" + str(lrModel.intercept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-02-15 13:40:03.140895\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Saved Data...\n",
      "Sample Records...\n",
      "+----------+--------------------+--------------------+-----+\n",
      "|prediction|            features|         probability|label|\n",
      "+----------+--------------------+--------------------+-----+\n",
      "|       0.0|[0.0,0.0,0.001062...|[0.87827002886590...|    0|\n",
      "|       0.0|[0.40762356689962...|[0.67405520613893...|    0|\n",
      "|       0.0|[0.0,0.0,7.440795...|[0.87832169649086...|    1|\n",
      "|       0.0|[0.0,0.0,0.526575...|[0.76454965013404...|    0|\n",
      "|       0.0|[0.0,0.0,0.066700...|[0.86720069076097...|    0|\n",
      "+----------+--------------------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "2018-02-15 13:40:04.332413\n"
     ]
    }
   ],
   "source": [
    "print ('Reading Saved Data...')\n",
    "train_predictions = sqlContext.read.parquet(\"gs://westfield-tom/datalab/train_predictions\")\n",
    "print ('Sample Records...')\n",
    "train_predictions.select(\"prediction\", \"features\", \"probability\", \"label\").show(5)\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-02-15 13:40:04.337918\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positive =  715775\n",
      "False Negative =  1766430\n",
      "True Negative =  7046137\n",
      "False Positive =  335019\n",
      "2018-02-15 13:40:30.455939\n"
     ]
    }
   ],
   "source": [
    "#Confusion matrix\n",
    "#Logic for getting confusion matrix and model evaluation parameters for Cross Validation Data\n",
    "TP = train_predictions.filter((col(\"prediction\") == 1) &  (col(\"label\")==1)).count()\n",
    "FP = train_predictions.filter((col(\"prediction\") == 1) &  (col(\"label\") == 0)).count()\n",
    "TN = train_predictions.filter((col(\"prediction\") == 0) &  (col(\"label\") == 0)).count()\n",
    "FN = train_predictions.filter((col(\"prediction\") == 0) &  (col(\"label\") == 1)).count()\n",
    "\n",
    "print 'True Positive = ', TP\n",
    "print 'False Negative = ', FN\n",
    "print 'True Negative = ', TN\n",
    "print 'False Positive = ', FP\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-02-15 13:40:30.461987\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+----------------+-------------------+--------------------+-------+-----------+--------------------+-------------------+--------------------+\n",
      "|row_number|   minimum|   maximum|number_of_target|     perc_of_target|number_of_non_target|cum_tar|cum_non_tar|     per_dist_target|per_dist_non_target|              spread|\n",
      "+----------+----------+----------+----------------+-------------------+--------------------+-------+-----------+--------------------+-------------------+--------------------+\n",
      "|         1|0.12155755|0.12156872|           82306| 0.0834462089997729|              904030|  82306|     904030|0.033158488437676256|0.12247813824233988| 0.08931964980466363|\n",
      "|         2|0.12156872| 0.1216542|          130398|0.13220444148849886|              855938| 212704|    1759968| 0.08569172508258803|0.23844076414067503|   0.152749039058087|\n",
      "|         3| 0.1216542|0.12251811|          171446| 0.1738210913927911|              814890| 384150|    2574858| 0.15476190476190477| 0.3488422000137106|  0.1940802952518058|\n",
      "|         4|0.12251811| 0.1338681|          151521|0.15362006456217758|              834815| 535671|    3409673| 0.21580493110949964| 0.4619430782774618| 0.24613814716796215|\n",
      "|         5| 0.1338681|0.18798345|          226471| 0.2296083687506083|              759865| 762142|    4169538|  0.3070429457739102| 0.5648897177866767|  0.2578467720127665|\n",
      "|         6|0.18798345|0.22935066|          141058| 0.1430121175745385|              845278| 903200|    5014816|  0.3638707598098461| 0.6794081250709577|  0.3155373652611116|\n",
      "|         7|0.22935066|0.26933298|          146523|0.14855282581189372|              839813|1049723|    5854629|  0.4229002497784224| 0.7931861332252382| 0.37028588344681584|\n",
      "|         8|0.26933298|0.35711998|          362681|0.36770532556856894|              623655|1412404|    6478284|  0.5690129723632261| 0.8776790187550618|  0.3086660463918357|\n",
      "|         9|0.35711998| 0.5142821|          385083|0.39041766700191416|              601253|1797487|    7079537|  0.7241507533639513| 0.9591368775126491|  0.2349861241486978|\n",
      "|        10| 0.5142824| 0.8864009|          684717|  0.694202584109269|              301619|2482204|    7381156|  1.0000016114736927| 1.0000002709603404|1.340513352321437...|\n",
      "+----------+----------+----------+----------------+-------------------+--------------------+-------+-----------+--------------------+-------------------+--------------------+\n",
      "\n",
      "2018-02-15 14:12:08.008089\n"
     ]
    }
   ],
   "source": [
    "# KS For Cross Validation\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "\n",
    "outlier_treated_cv=train\n",
    "\n",
    "split1_udf = udf(lambda value: value[0].item(), FloatType())\n",
    "split2_udf = udf(lambda value: value[1].item(), FloatType())\n",
    "prediction_prob = train_predictions.select(split1_udf('probability').alias('Prob_0'), split2_udf('probability').alias('Prob_1'),\"label\")\n",
    "\n",
    "\n",
    "# Size for each bins\n",
    "bin_size = float(outlier_treated_cv.count()/10)\n",
    "# Total Events\n",
    "total_events = float(str(outlier_treated_cv.select(sum(\"target\")).head()).replace(\")\",\"\").split(\"=\")[1])\n",
    "# Total non events\n",
    "total_non_events = outlier_treated_cv.count() - float(str(outlier_treated_cv.select(sum(\"Target\")).head()).replace(\")\",\"\").split(\"=\")[1])\n",
    "\n",
    "# Decile Tables used for KS\n",
    "x = prediction_prob.withColumn(\"row_number\",ceil(F.row_number().over(Window.partitionBy().orderBy(\"prob_1\"))/bin_size)).groupBy(\"row_number\").agg(min(\"prob_1\").alias(\"minimum\"),max(\"prob_1\").alias(\"maximum\"),sum(\"label\").alias(\"number_of_target\"), (sum(\"label\")/int(bin_size)).alias(\"perc_of_target\"),(int(bin_size)-sum(\"label\")).alias(\"number_of_non_target\"))\n",
    "x= x.filter(col(\"row_number\")<=10).withColumn(\"cum_tar\", sum(\"number_of_target\").over(Window.partitionBy().orderBy(\"row_number\"))).withColumn(\"cum_non_tar\", sum(\"number_of_non_target\").over(Window.partitionBy().orderBy(\"row_number\"))).withColumn(\"per_dist_target\",col(\"cum_tar\")/total_events).withColumn(\"per_dist_non_target\",col(\"cum_non_tar\")/total_non_events).withColumn(\"spread\",abs(col(\"per_dist_target\")-col(\"per_dist_non_target\")))\n",
    "x.show()\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
