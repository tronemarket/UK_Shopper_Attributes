{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UK Loyalty Indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime \n",
    "from google.cloud import bigquery\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import col,udf,array\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "\n",
    "from pyspark.sql.functions import col,udf,array\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-02-15 09:36:19.977787\n"
     ]
    }
   ],
   "source": [
    "import datetime;print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-02-15 09:36:20.670759\n"
     ]
    }
   ],
   "source": [
    "# Exporting BQ Tables to Google Storage\n",
    "\n",
    "bigquery_client = bigquery.Client()\n",
    "dataset_ref = bigquery_client.dataset(\"tom\") # DATASET OF BQ\n",
    "table_ref = dataset_ref.table(\"Yocuda_clean_data_Nov15_Nov17_20171214_v01\") # TABLE NAME OF BQ\n",
    "\n",
    "\n",
    "job = bigquery_client.extract_table(table_ref, 'gs://westfield-tom/datalab/Yocuda_clean_data_Nov15_Nov17_20171214_v01_*.csv') # GOOGLE CLOUD BUCKET\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-02-15 09:36:20.676615\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-02-15 09:36:53.112431\n"
     ]
    }
   ],
   "source": [
    "#Importing \n",
    "data = (spark.read.format(\"csv\")\n",
    "      .option(\"header\", \"true\")\n",
    "      .load('gs://westfield-tom/datalab/Yocuda_clean_data_Nov15_Nov17_20171214_v01_*.csv'))\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-02-15 09:36:53.117959\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|          identifier|\n",
      "+--------------------+\n",
      "|sazidchoudhury@gm...|\n",
      "|shakur_b9@hotmail...|\n",
      "|michaelharteis@ho...|\n",
      "|  rebekah869@msn.com|\n",
      "|     skorp@gmail.com|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "2018-02-15 09:39:02.225298\n"
     ]
    }
   ],
   "source": [
    "# Overall - list of unique indicators\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "overall = data.filter((to_date(col(\"timestamp\"))>'2016-11-30') & (to_date(col(\"timestamp\"))<'2017-12-01') & (col(\"retailer_name\")=='Argos')).select(\"identifier\").distinct()\n",
    "overall.show(5)\n",
    "overall.count()\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-02-15 09:39:02.231004\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+\n",
      "|          identifier|    brand_diversity|\n",
      "+--------------------+-------------------+\n",
      "|   -.tmh24@gmail.com|                0.0|\n",
      "|-indiaismial78672...|                0.0|\n",
      "|.j.douglas@hotmai...|                0.0|\n",
      "|    00673@uk.mcd.com|                0.0|\n",
      "|00blackswan7@yaho...|0.22891523252096496|\n",
      "+--------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "2018-02-15 09:42:49.232262\n"
     ]
    }
   ],
   "source": [
    "# Brand\n",
    "\n",
    "df1 = data.filter((to_date(col(\"timestamp\"))>'2016-11-30') & \\\n",
    "                  (to_date(col(\"timestamp\"))<'2017-12-01') & \\\n",
    "                  (col(\"brandName\").isNotNull()) & \\\n",
    "                  (col(\"identifier\").isNotNull()) & \\\n",
    "                  (col(\"item_total\")>0) & \\\n",
    "                  (col(\"retailer_name\")=='Argos'))\\\n",
    ".groupBy(\"identifier\",\"brandName\").agg(sum(\"item_total\").alias(\"spend\"))\n",
    "\n",
    "\n",
    "df2 = data.filter((to_date(col(\"timestamp\"))>'2016-11-30') & \\\n",
    "                  (to_date(col(\"timestamp\"))<'2017-12-01') & \\\n",
    "                  (col(\"brandName\").isNotNull()) & \\\n",
    "                  (col(\"identifier\").isNotNull()) & \\\n",
    "                  (col(\"item_total\")>0) & \\\n",
    "                  (col(\"retailer_name\")=='Argos'))\\\n",
    ".groupBy(\"identifier\").agg(sum(\"item_total\").alias(\"spend_overall\"))\n",
    "\n",
    "df3 = df1.join(df2,[\"identifier\"],\"left\")\n",
    "\n",
    "brand = df3.withColumn('entropy', -(col('spend')/ col('spend_overall'))*log10(col('spend')/col('spend_overall')))\\\n",
    ".groupBy(col(\"identifier\")).agg(sum(\"entropy\").alias(\"brand_diversity\"))\n",
    "\n",
    "brand.show(5)\n",
    "brand.count()\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-02-15 09:42:49.238452\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+\n",
      "|          identifier| category_diversity|\n",
      "+--------------------+-------------------+\n",
      "|   -.tmh24@gmail.com|                0.0|\n",
      "|-indiaismial78672...|  0.299594933046483|\n",
      "|.j.douglas@hotmai...|                0.0|\n",
      "|    00673@uk.mcd.com| 0.2863175935284513|\n",
      "|00blackswan7@yaho...|0.22891523252096496|\n",
      "+--------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "2018-02-15 09:46:29.882627\n"
     ]
    }
   ],
   "source": [
    "# category\n",
    "\n",
    "df1 = data.filter((to_date(col(\"timestamp\"))>'2016-11-30') & \\\n",
    "                  (to_date(col(\"timestamp\"))<'2017-12-01') & \\\n",
    "                  (col(\"cat2\").isNotNull()) & \\\n",
    "                  (col(\"identifier\").isNotNull()) & \\\n",
    "                  (col(\"item_total\")>0) & \\\n",
    "                  (col(\"retailer_name\")=='Argos'))\\\n",
    ".groupBy(\"identifier\",\"cat2\").agg(sum(\"item_total\").alias(\"spend\"))\n",
    "\n",
    "\n",
    "df2 = data.filter((to_date(col(\"timestamp\"))>'2016-11-30') & \\\n",
    "                  (to_date(col(\"timestamp\"))<'2017-12-01') & \\\n",
    "                  (col(\"cat2\").isNotNull()) & \\\n",
    "                  (col(\"identifier\").isNotNull()) & \\\n",
    "                  (col(\"item_total\")>0) & \\\n",
    "                  (col(\"retailer_name\")=='Argos'))\\\n",
    ".groupBy(\"identifier\").agg(sum(\"item_total\").alias(\"spend_overall\"))\n",
    "\n",
    "df3 = df1.join(df2,[\"identifier\"],\"left\")\n",
    "\n",
    "category = df3.withColumn('entropy', -(col('spend')/ col('spend_overall'))*log10(col('spend')/col('spend_overall')))\\\n",
    ".groupBy(col(\"identifier\")).agg(sum(\"entropy\").alias(\"category_diversity\"))\n",
    "\n",
    "category.show(5)\n",
    "category.count()\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-02-15 09:46:29.889011\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+\n",
      "|          identifier|         timeseries|\n",
      "+--------------------+-------------------+\n",
      "|andycooksupply@ho...|0.05852528701915769|\n",
      "|judyfashions5@gma...| 1.7594520520367305|\n",
      "|michellearnold600...|0.22629717562804758|\n",
      "|pspearpoint@yahoo...| 0.6085991155504794|\n",
      "|typeitright@bluey...| 2.5440432268136592|\n",
      "+--------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "2018-02-15 09:53:17.592476\n"
     ]
    }
   ],
   "source": [
    "data.registerTempTable(\"data\")\n",
    "\n",
    "xx1 =data.selectExpr(\"identifier\",\"month(timestamp) as month\", \"year(timestamp) as year\", \"item_total\", \"1 as ind\").filter((to_date(col(\"timestamp\"))>'2016-11-30') & \\\n",
    "                  (to_date(col(\"timestamp\"))<'2017-12-01') & \\\n",
    "                  (col(\"identifier\").isNotNull()) & \\\n",
    "                  (col(\"item_total\")>0) & \\\n",
    "                  (col(\"retailer_name\")=='Argos'))\n",
    "\n",
    "xx2 =data.selectExpr(\"timestamp \", \"1 as ind\").filter((to_date(col(\"timestamp\"))>'2016-11-30') & \\\n",
    "                  (to_date(col(\"timestamp\"))<'2017-12-01') & \\\n",
    "                  (col(\"identifier\").isNotNull()) & \\\n",
    "                  (col(\"item_total\")>0) & \\\n",
    "                  (col(\"retailer_name\")=='Argos'))\\\n",
    ".groupBy(col(\"ind\")).agg(max(\"timestamp\").alias(\"timestamp_mx\"))\n",
    "\n",
    "xx3 = xx1.join(xx2,[\"ind\"],\"left\")\n",
    "\n",
    "xx4 = xx3.selectExpr(\"identifier\",\"((month(timestamp_mx) - month)+ (year(timestamp_mx)- year)) as recency\", \"item_total\")\\\n",
    ".groupBy(col(\"identifier\"),col(\"recency\")).agg(sum(\"item_total\").alias(\"spend\"))\n",
    "\n",
    "\n",
    "timeseries = xx4.selectExpr(\"identifier\", \"((exp((-1) * recency)) * log10(spend)) as timeseries\")\\\n",
    ".groupBy(col(\"identifier\")).agg(sum(\"timeseries\").alias(\"timeseries\"))\n",
    "\n",
    "timeseries.show(5)\n",
    "timeseries.count()\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-02-15 09:53:17.599171\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+--------------------+\n",
      "|          identifier|    brand_diversity| category_diversity|          timeseries|\n",
      "+--------------------+-------------------+-------------------+--------------------+\n",
      "|   -.tmh24@gmail.com|                0.0|                0.0|0.002840198153859...|\n",
      "|-indiaismial78672...|                0.0|  0.299594933046483|8.485499381124429E-5|\n",
      "|.j.douglas@hotmai...|                0.0|                0.0|0.021535558112033094|\n",
      "|    00673@uk.mcd.com|                0.0| 0.2863175935284513|2.288245512950547E-4|\n",
      "|00blackswan7@yaho...|0.22891523252096496|0.22891523252096496|1.813540983064614...|\n",
      "+--------------------+-------------------+-------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "2018-02-15 10:06:45.716609\n"
     ]
    }
   ],
   "source": [
    "combined = overall.join(brand,[\"identifier\"],\"left\").join(category,[\"identifier\"],\"left\").join(timeseries,[\"identifier\"],\"left\")\n",
    "\n",
    "combined.show(5)\n",
    "combined.count()\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-02-15 10:06:45.722780\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+--------------------+\n",
      "|          identifier|    brand_diversity| category_diversity|          timeseries|\n",
      "+--------------------+-------------------+-------------------+--------------------+\n",
      "|   -.tmh24@gmail.com|                0.0|                0.0|0.002840198153859...|\n",
      "|-indiaismial78672...|                0.0|  0.299594933046483|8.485499381124429E-5|\n",
      "|.j.douglas@hotmai...|                0.0|                0.0|0.021535558112033094|\n",
      "|    00673@uk.mcd.com|                0.0| 0.2863175935284513|2.288245512950547E-4|\n",
      "|00blackswan7@yaho...|0.22891523252096496|0.22891523252096496|1.813540983064614...|\n",
      "+--------------------+-------------------+-------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "2018-02-15 10:34:35.313893\n"
     ]
    }
   ],
   "source": [
    "# Importing the data and starting the process\n",
    "data_new = combined\n",
    "\n",
    "# Replacing the missing values by 0 \n",
    "# Although it's not necessary as there are no missing values most of the time\n",
    "data_new = data_new.fillna(0)\n",
    "\n",
    "# Creating spark sql tables for faster extraction of the percentiles\n",
    "data_new.createOrReplaceTempView('temporary_intermediate_table')\n",
    "\n",
    "# Storing the column names in a set (ordered list of sorts)\n",
    "col_names = set(data_new.columns) - {\"identifier\"}\n",
    "\n",
    "# Writing queries to calculate the 99th (upper limit) and 1st (lower limit) percentile\n",
    "upper_limit_query = 'SELECT ' + ', '.join(['Percentile({var_name}, 0.99) AS {var_name}'.format(var_name = i) for i in col_names]) + ' from ' \n",
    "lower_limit_query = 'SELECT ' + ', '.join(['Percentile({var_name}, 0.01) AS {var_name}'.format(var_name = i) for i in col_names]) + ' from ' \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Saving the capping limits in two pandas DataFrames \n",
    "upper_limit = spark.sql(upper_limit_query + 'temporary_intermediate_table').toPandas()\n",
    "lower_limit = spark.sql(lower_limit_query + 'temporary_intermediate_table').toPandas()\n",
    "\n",
    "# Deciding which columns need outlier treatment (Dropping those which have 1st and 99th percentile as same)\n",
    "columns_to_treat = {var_name for var_name in col_names if upper_limit.iloc[0][var_name] > lower_limit.iloc[0][var_name]}\n",
    "\n",
    "# Writing a SQL query for outlier treatment\n",
    "outlier_treated_data = spark.sql('SELECT identifier, '+ ', '.join(['IF({var_name} >= {maximum}, {maximum}, IF({var_name} <={minimum}, {minimum}, {var_name})) AS  {var_name}'.format(var_name = var_name, maximum = upper_limit.iloc[0][var_name], minimum = lower_limit.iloc[0][var_name])  for var_name in columns_to_treat]) + ' FROM ' + 'temporary_intermediate_table')\n",
    "\n",
    "outlier_treated_data.show(5)\n",
    "outlier_treated_data.count()\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-02-15 10:42:04.141213\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-02-15 10:57:10.178770\n"
     ]
    }
   ],
   "source": [
    "#Saving data into GCS\n",
    "(outlier_treated_data.coalesce(1).write.option(\"header\", \"true\").mode(\"overwrite\").csv(\"gs://westfield-tom/datalab/Loyalty_indicator/outlier_treated_data\"))\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-02-20 12:26:35.490653\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+--------------------+\n",
      "|          identifier|    brand_diversity| category_diversity|          timeseries|\n",
      "+--------------------+-------------------+-------------------+--------------------+\n",
      "|   -.tmh24@gmail.com|                0.0|                0.0|0.002840198153859...|\n",
      "|-indiaismial78672...|                0.0|  0.299594933046483|8.485499381124429E-5|\n",
      "|.j.douglas@hotmai...|                0.0|                0.0|0.021535558112033094|\n",
      "|    00673@uk.mcd.com|                0.0| 0.2863175935284513|2.288245512950547E-4|\n",
      "|00blackswan7@yaho...|0.22891523252096496|0.22891523252096496|1.813540983064614...|\n",
      "+--------------------+-------------------+-------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "2018-02-20 12:46:40.974361\n"
     ]
    }
   ],
   "source": [
    "# Reading data in from GCS\n",
    "outlier_treated_data = (spark.read.format(\"csv\")\n",
    "      .option(\"header\", \"true\").option(\"inferSchema\", \"true\")\n",
    "      .load('gs://westfield-tom/datalab/Loyalty_indicator/outlier_treated_data/part-00000-9896de6e-3eeb-4178-bf6c-a6c368cdeec4-c000.csv'))\n",
    "\n",
    "outlier_treated_data.show(5)\n",
    "outlier_treated_data.count()\n",
    "\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('identifier', 'string'),\n",
       " ('brand_diversity', 'double'),\n",
       " ('category_diversity', 'double'),\n",
       " ('timeseries', 'double')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outlier_treated_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-02-20 12:47:04.721610\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+--------------------+\n",
      "|          identifier|    brand_diversity| category_diversity|          timeseries|\n",
      "+--------------------+-------------------+-------------------+--------------------+\n",
      "|   -.tmh24@gmail.com|                0.0|                0.0|9.893906597435384E-4|\n",
      "|-indiaismial78672...|                0.0|0.34327102441366086|2.955946513638719E-5|\n",
      "|.j.douglas@hotmai...|                0.0|                0.0|0.007501969543729179|\n",
      "|    00673@uk.mcd.com|                0.0| 0.3280580637287231|7.971164739463231E-5|\n",
      "|00blackswan7@yaho...|0.34604067677403916| 0.2622873677211728|6.317518752232143E-5|\n",
      "+--------------------+-------------------+-------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "2018-02-20 12:47:29.878432\n"
     ]
    }
   ],
   "source": [
    "# NORMALIZATION: TARGET is at 0th INDEX\n",
    "from pyspark.sql.functions import col,udf,array\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "\n",
    "# NORMALIZATION: TARGET is at 0th INDEX\n",
    "from pyspark.sql.functions import col,udf,array\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "\n",
    "\n",
    "# Defining the function to normalize the data.\n",
    "def normalization_missing_value(data_new):\n",
    "  i = 0\n",
    "#   Transforming the variables as per Maximum and Minimum Value\n",
    "  for cols in data_new.columns[1:]:\n",
    "    maxs = data_new.agg(max(col(cols)).alias(\"max\")).toPandas()\n",
    "    mins = data_new.agg(min(col(cols)).alias(\"min\")).toPandas()\n",
    "    data_new = data_new.withColumn(cols,((col(cols)-mins[\"min\"][0])/(maxs[\"max\"][0]-mins[\"min\"][0])))\n",
    "    i = i+1\n",
    "  return data_new\n",
    "                           \n",
    "final= normalization_missing_value(outlier_treated_data)\n",
    "final.show(5)\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-02-20 12:50:09.799126\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-02-20 12:51:25.404590\n"
     ]
    }
   ],
   "source": [
    "#Saving data into GCS\n",
    "(final.coalesce(1).write.option(\"header\", \"true\").mode(\"overwrite\").csv(\"gs://westfield-tom/datalab/Loyalty_indicator/normalised_metrics\"))\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-02-15 11:42:54.105759\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+-------+------------------+\n",
      "|bucket|               spend|cnt_shp|         avg_spend|\n",
      "+------+--------------------+-------+------------------+\n",
      "|  0-10|1.1849354187978327E9|2106163| 562.6038529771118|\n",
      "| 10-20|4.2566265366426456E8|2131922|199.66145743806038|\n",
      "| 20-30|2.5298825609055772E8|2081822|121.52252022053649|\n",
      "| 30-40| 1.754814611097583E8|2122119| 82.69162149236602|\n",
      "| 40-50|1.2177078294859919E8|2095970| 58.09757913930027|\n",
      "| 50-60| 8.995130797959337E7|2099623|42.841647276484096|\n",
      "| 60-70| 6.702846659015419E7|2104122| 31.85578906078364|\n",
      "| 70-80| 5.454329904064297E7|2382295|22.895274951524883|\n",
      "| 80-90|3.0570063189468637E7|1833997|16.668545907909685|\n",
      "|90-100|2.1033439779639587E7|2099218|10.019654833199594|\n",
      "+------+--------------------+-------+------------------+\n",
      "\n",
      "2018-02-15 11:45:15.832439\n"
     ]
    }
   ],
   "source": [
    "################################################ Defining Target Variable ############################################\n",
    "\n",
    "# Spend Based Model \n",
    "from pyspark.sql.window import Window\n",
    "df1 = data.filter((to_date(col(\"timestamp\"))>'2016-11-30') & \\\n",
    "                  (to_date(col(\"timestamp\"))<'2017-12-01') & \\\n",
    "                  (col(\"identifier\").isNotNull()) & \\\n",
    "                  (col(\"item_total\")>0))\\\n",
    ".groupBy(\"identifier\").agg(sum(\"item_total\").alias(\"spend\"))\n",
    "\n",
    "df2 = df1.withColumn('percentile', percent_rank().over(Window.orderBy(desc('spend')))) \\\n",
    ".select('identifier','spend', 'percentile')\n",
    "\n",
    "df3 = df2.withColumn(\"bucket\", (when(col(\"percentile\")<=0.10,\"0-10\").otherwise(\\\n",
    "      when(col('percentile')<=0.20,\"10-20\").otherwise(\\\n",
    "        when(col('percentile')<=0.30,\"20-30\").otherwise(\\\n",
    "          when(col('percentile')<=0.40,\"30-40\").otherwise(\\\n",
    "            when(col('percentile')<=0.50,\"40-50\").otherwise(\\\n",
    "              when(col('percentile')<=0.60,\"50-60\").otherwise(\\\n",
    "                when(col('percentile')<=0.70,\"60-70\").otherwise(\\\n",
    "                  when(col('percentile')<=0.80,\"70-80\").otherwise(\\\n",
    "                    when(col('percentile')<=0.90,\"80-90\").otherwise('90-100')))))))))))\\\n",
    "\n",
    "output = df3.groupBy(\"bucket\").agg((sum(col(\"spend\")).alias(\"spend\"))\\\n",
    "                       ,(countDistinct(col(\"identifier\")).alias(\"cnt_shp\"))\\\n",
    "                       ,(avg(col(\"spend\")).alias(\"avg_spend\")))\n",
    "\n",
    "output.show()\n",
    "print(datetime.datetime.now())\n",
    "######################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-02-15 11:45:15.838643\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+--------------------+------+\n",
      "|          identifier|    brand_diversity| category_diversity|          timeseries|target|\n",
      "+--------------------+-------------------+-------------------+--------------------+------+\n",
      "|   -.tmh24@gmail.com|                0.0|                0.0|9.893906597435384E-4|     0|\n",
      "|-indiaismial78672...|                0.0|0.34327102441366086|2.955946513638719E-5|     0|\n",
      "|.j.douglas@hotmai...|                0.0|                0.0|0.007501969543729179|     0|\n",
      "|    00673@uk.mcd.com|                0.0| 0.3280580637287231|7.971164739463231E-5|     0|\n",
      "|00blackswan7@yaho...|0.34604067677403916| 0.2622873677211728|6.317518752232143E-5|     0|\n",
      "+--------------------+-------------------+-------------------+--------------------+------+\n",
      "only showing top 5 rows\n",
      "\n",
      "2018-02-15 11:52:45.428069\n"
     ]
    }
   ],
   "source": [
    "################################################ Defining Target Variable ############################################\n",
    "\n",
    "# Spend Based Model \n",
    "from pyspark.sql.window import Window\n",
    "df1 = data.filter((to_date(col(\"timestamp\"))>'2016-11-30') & \\\n",
    "                  (to_date(col(\"timestamp\"))<'2017-12-01') & \\\n",
    "                  (col(\"identifier\").isNotNull()) & \\\n",
    "                  (col(\"item_total\")>0))\\\n",
    ".groupBy(\"identifier\").agg(sum(\"item_total\").alias(\"spend\"))\n",
    "\n",
    "df2 = df1.withColumn('percentile', percent_rank().over(Window.orderBy(desc('spend')))) \\\n",
    ".select('identifier','spend', 'percentile')\n",
    "\n",
    "target = df2.withColumn(\"target\",(when(col(\"percentile\")<=0.20,1).otherwise(0)))\\\n",
    "                     .select('identifier','target')\n",
    "\n",
    "final_w_target = final.join(target,[\"identifier\"],\"left\").fillna(0)\n",
    "\n",
    "final_w_target.show(5)\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-02-15 11:52:45.496895\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+------------------+--------------------+------+\n",
      "|          identifier|    brand_diversity|category_diversity|          timeseries|target|\n",
      "+--------------------+-------------------+------------------+--------------------+------+\n",
      "|.j.douglas@hotmai...|                0.0|               0.0|0.007501969543729179|     0|\n",
      "|    00673@uk.mcd.com|                0.0|0.3280580637287231|7.971164739463231E-5|     0|\n",
      "|00blackswan7@yaho...|0.34604067677403916|0.2622873677211728|6.317518752232143E-5|     0|\n",
      "| 01388dave@gmail.com|                0.0|               0.0|  0.7242505480831564|     0|\n",
      "|01656858872@talkt...| 0.8913519382514233|0.8211887484951836|  0.7681917813342942|     1|\n",
      "+--------------------+-------------------+------------------+--------------------+------+\n",
      "only showing top 5 rows\n",
      "\n",
      "2018-02-15 12:00:22.758377\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "import random\n",
    "random.seed(7)\n",
    "train, test = final_w_target.randomSplit([0.8, 0.2])\n",
    "\n",
    "train.show(5)\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-02-15 12:00:22.764122\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-02-15 12:16:37.918147\n"
     ]
    }
   ],
   "source": [
    "####### Saving Test and train dataset ############\n",
    "\n",
    "train.write.mode(\"overwrite\").parquet(\"gs://westfield-tom/datalab/train_set2\")\n",
    "test.write.mode(\"overwrite\").parquet(\"gs://westfield-tom/datalab/test_set2\")\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################################################################################################################\n",
    "################################################## RUN Data Proc Code ################################################\n",
    "######################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################ To Run After training the model  ##################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Coeffecients and Intercept for Logistic Regression ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-02-15 13:40:00.913915\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Saved Model...\n",
      "Model Coeffecients...\n",
      "Coefficients: [0.9813838992695821,1.531667952505228,1.5192042104267771]\n",
      "Intercept: -1.97776262668\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import *\n",
    "print ('Reading Saved Model...')\n",
    "\n",
    "lrModel = LogisticRegressionModel.read().load(\"gs://westfield-tom/datalab/lr_model1\")\n",
    "\n",
    "# Print the coefficients and intercept for logistic regression\n",
    "print ('Model Coeffecients...')\n",
    "print(\"Coefficients: \" + str(lrModel.coefficients))\n",
    "print(\"Intercept: \" + str(lrModel.intercept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-02-15 13:40:03.140895\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Saved Data...\n",
      "Sample Records...\n",
      "+----------+--------------------+--------------------+-----+\n",
      "|prediction|            features|         probability|label|\n",
      "+----------+--------------------+--------------------+-----+\n",
      "|       0.0|[0.0,0.0,0.001062...|[0.87827002886590...|    0|\n",
      "|       0.0|[0.40762356689962...|[0.67405520613893...|    0|\n",
      "|       0.0|[0.0,0.0,7.440795...|[0.87832169649086...|    1|\n",
      "|       0.0|[0.0,0.0,0.526575...|[0.76454965013404...|    0|\n",
      "|       0.0|[0.0,0.0,0.066700...|[0.86720069076097...|    0|\n",
      "+----------+--------------------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "2018-02-15 13:40:04.332413\n"
     ]
    }
   ],
   "source": [
    "print ('Reading Saved Data...')\n",
    "train_predictions = sqlContext.read.parquet(\"gs://westfield-tom/datalab/train_predictions\")\n",
    "print ('Sample Records...')\n",
    "train_predictions.select(\"prediction\", \"features\", \"probability\", \"label\").show(5)\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-02-15 13:40:04.337918\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positive =  715775\n",
      "False Negative =  1766430\n",
      "True Negative =  7046137\n",
      "False Positive =  335019\n",
      "2018-02-15 13:40:30.455939\n"
     ]
    }
   ],
   "source": [
    "#Confusion matrix\n",
    "#Logic for getting confusion matrix and model evaluation parameters for Cross Validation Data\n",
    "TP = train_predictions.filter((col(\"prediction\") == 1) &  (col(\"label\")==1)).count()\n",
    "FP = train_predictions.filter((col(\"prediction\") == 1) &  (col(\"label\") == 0)).count()\n",
    "TN = train_predictions.filter((col(\"prediction\") == 0) &  (col(\"label\") == 0)).count()\n",
    "FN = train_predictions.filter((col(\"prediction\") == 0) &  (col(\"label\") == 1)).count()\n",
    "\n",
    "print 'True Positive = ', TP\n",
    "print 'False Negative = ', FN\n",
    "print 'True Negative = ', TN\n",
    "print 'False Positive = ', FP\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-02-15 13:40:30.461987\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+----------------+-------------------+--------------------+-------+-----------+--------------------+-------------------+--------------------+\n",
      "|row_number|   minimum|   maximum|number_of_target|     perc_of_target|number_of_non_target|cum_tar|cum_non_tar|     per_dist_target|per_dist_non_target|              spread|\n",
      "+----------+----------+----------+----------------+-------------------+--------------------+-------+-----------+--------------------+-------------------+--------------------+\n",
      "|         1|0.12155755|0.12156872|           82306| 0.0834462089997729|              904030|  82306|     904030|0.033158488437676256|0.12247813824233988| 0.08931964980466363|\n",
      "|         2|0.12156872| 0.1216542|          130398|0.13220444148849886|              855938| 212704|    1759968| 0.08569172508258803|0.23844076414067503|   0.152749039058087|\n",
      "|         3| 0.1216542|0.12251811|          171446| 0.1738210913927911|              814890| 384150|    2574858| 0.15476190476190477| 0.3488422000137106|  0.1940802952518058|\n",
      "|         4|0.12251811| 0.1338681|          151521|0.15362006456217758|              834815| 535671|    3409673| 0.21580493110949964| 0.4619430782774618| 0.24613814716796215|\n",
      "|         5| 0.1338681|0.18798345|          226471| 0.2296083687506083|              759865| 762142|    4169538|  0.3070429457739102| 0.5648897177866767|  0.2578467720127665|\n",
      "|         6|0.18798345|0.22935066|          141058| 0.1430121175745385|              845278| 903200|    5014816|  0.3638707598098461| 0.6794081250709577|  0.3155373652611116|\n",
      "|         7|0.22935066|0.26933298|          146523|0.14855282581189372|              839813|1049723|    5854629|  0.4229002497784224| 0.7931861332252382| 0.37028588344681584|\n",
      "|         8|0.26933298|0.35711998|          362681|0.36770532556856894|              623655|1412404|    6478284|  0.5690129723632261| 0.8776790187550618|  0.3086660463918357|\n",
      "|         9|0.35711998| 0.5142821|          385083|0.39041766700191416|              601253|1797487|    7079537|  0.7241507533639513| 0.9591368775126491|  0.2349861241486978|\n",
      "|        10| 0.5142824| 0.8864009|          684717|  0.694202584109269|              301619|2482204|    7381156|  1.0000016114736927| 1.0000002709603404|1.340513352321437...|\n",
      "+----------+----------+----------+----------------+-------------------+--------------------+-------+-----------+--------------------+-------------------+--------------------+\n",
      "\n",
      "2018-02-15 14:12:08.008089\n"
     ]
    }
   ],
   "source": [
    "# KS For Cross Validation\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "\n",
    "outlier_treated_cv=train\n",
    "\n",
    "split1_udf = udf(lambda value: value[0].item(), FloatType())\n",
    "split2_udf = udf(lambda value: value[1].item(), FloatType())\n",
    "prediction_prob = train_predictions.select(split1_udf('probability').alias('Prob_0'), split2_udf('probability').alias('Prob_1'),\"label\")\n",
    "\n",
    "\n",
    "# Size for each bins\n",
    "bin_size = float(outlier_treated_cv.count()/10)\n",
    "# Total Events\n",
    "total_events = float(str(outlier_treated_cv.select(sum(\"target\")).head()).replace(\")\",\"\").split(\"=\")[1])\n",
    "# Total non events\n",
    "total_non_events = outlier_treated_cv.count() - float(str(outlier_treated_cv.select(sum(\"Target\")).head()).replace(\")\",\"\").split(\"=\")[1])\n",
    "\n",
    "# Decile Tables used for KS\n",
    "x = prediction_prob.withColumn(\"row_number\",ceil(F.row_number().over(Window.partitionBy().orderBy(\"prob_1\"))/bin_size)).groupBy(\"row_number\").agg(min(\"prob_1\").alias(\"minimum\"),max(\"prob_1\").alias(\"maximum\"),sum(\"label\").alias(\"number_of_target\"), (sum(\"label\")/int(bin_size)).alias(\"perc_of_target\"),(int(bin_size)-sum(\"label\")).alias(\"number_of_non_target\"))\n",
    "x= x.filter(col(\"row_number\")<=10).withColumn(\"cum_tar\", sum(\"number_of_target\").over(Window.partitionBy().orderBy(\"row_number\"))).withColumn(\"cum_non_tar\", sum(\"number_of_non_target\").over(Window.partitionBy().orderBy(\"row_number\"))).withColumn(\"per_dist_target\",col(\"cum_tar\")/total_events).withColumn(\"per_dist_non_target\",col(\"cum_non_tar\")/total_non_events).withColumn(\"spread\",abs(col(\"per_dist_target\")-col(\"per_dist_non_target\")))\n",
    "x.show()\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################################################################################################################\n",
    "# Scoring Loyalty Indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+--------------------+\n",
      "|          identifier|    brand_diversity| category_diversity|          timeseries|\n",
      "+--------------------+-------------------+-------------------+--------------------+\n",
      "|   -.tmh24@gmail.com|                0.0|                0.0|9.893906597435384E-4|\n",
      "|-indiaismial78672...|                0.0|0.34327102441366086|2.955946513638719E-5|\n",
      "|.j.douglas@hotmai...|                0.0|                0.0|0.007501969543729179|\n",
      "|    00673@uk.mcd.com|                0.0| 0.3280580637287231|7.971164739463231E-5|\n",
      "|00blackswan7@yaho...|0.34604067677403916| 0.2622873677211728|6.317518752232143E-5|\n",
      "+--------------------+-------------------+-------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# brand_diversity = 0.243383328\n",
    "# category_diversity = 0.379853841\n",
    "# timeseries = 0.376762831"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+--------------------+--------------------+-----------------------+--------------------+--------------------+\n",
      "|          identifier|    brand_diversity| category_diversity|          timeseries|brand_diversity_calc|category_diversity_calc|     timeseries_calc|       loyalty_score|\n",
      "+--------------------+-------------------+-------------------+--------------------+--------------------+-----------------------+--------------------+--------------------+\n",
      "|   -.tmh24@gmail.com|                0.0|                0.0|9.893906597435384E-4|                 0.0|                    0.0|3.727656259299332...|3.727656259299332...|\n",
      "|-indiaismial78672...|                0.0|0.34327102441366086|2.955946513638719E-5|                 0.0|    0.13039281712753387|1.113690776763104E-5|  0.1304039540353015|\n",
      "|.j.douglas@hotmai...|                0.0|                0.0|0.007501969543729179|                 0.0|                    0.0|0.002826463283371...|0.002826463283371...|\n",
      "|    00673@uk.mcd.com|                0.0| 0.3280580637287231|7.971164739463231E-5|                 0.0|    0.12461411557837826|3.003238593607544...| 0.12464414796431433|\n",
      "|00blackswan7@yaho...|0.34604067677403916| 0.2622873677211728|6.317518752232143E-5| 0.08422053153663796|    0.09963086407466691| 2.38020624998657E-5| 0.18387519767380472|\n",
      "+--------------------+-------------------+-------------------+--------------------+--------------------+-----------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loyalty_score=final.withColumn(\"brand_diversity_calc\", when((col(\"brand_diversity\").isNotNull()),col(\"brand_diversity\")*0.243383328).otherwise(0)) \\\n",
    "                    .withColumn(\"category_diversity_calc\", when((col(\"category_diversity\").isNotNull()),col(\"category_diversity\")*0.379853841).otherwise(0)) \\\n",
    "                    .withColumn(\"timeseries_calc\", when((col(\"timeseries\").isNotNull()),col(\"timeseries\")*0.376762831).otherwise(0)) \\\n",
    "                    .withColumn(\"loyalty_score\", col(\"brand_diversity_calc\")+col(\"category_diversity_calc\")+col(\"timeseries_calc\")) \n",
    "loyalty_score.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------------+\n",
      "|          identifier|       loyalty_score|loyalty_tier|\n",
      "+--------------------+--------------------+------------+\n",
      "|   -.tmh24@gmail.com|3.727656259299332...|5.Occasional|\n",
      "|-indiaismial78672...|  0.1304039540353015|5.Occasional|\n",
      "|.j.douglas@hotmai...|0.002826463283371...|5.Occasional|\n",
      "|    00673@uk.mcd.com| 0.12464414796431433|5.Occasional|\n",
      "|00blackswan7@yaho...| 0.18387519767380472|5.Occasional|\n",
      "| 01388dave@gmail.com| 0.27287068684911164|    4.Bronze|\n",
      "|01555664963@talkt...|  0.7426570751179802|  1.Platinum|\n",
      "|01656858872@talkt...|   0.818298011739202|  1.Platinum|\n",
      "|       01ian@live.uk| 0.10694529543243954|5.Occasional|\n",
      "|01tinalou@btinter...|1.332780514470631E-4|5.Occasional|\n",
      "+--------------------+--------------------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loyalty_score_band=loyalty_score.selectExpr('identifier', 'loyalty_score').withColumn(\"loyalty_tier\", when((col(\"loyalty_score\")<=0.205438),\"5.Occasional\") \\\n",
    "                                .when((col(\"loyalty_score\")<=0.383549),\"4.Bronze\") \\\n",
    "                                .when((col(\"loyalty_score\")<=0.544249),\"3.Silver\") \\\n",
    "                                .when((col(\"loyalty_score\")<=0.673125),\"2.Gold\") \\\n",
    "                                .when((col(\"loyalty_score\")>0.673125),\"1.Platinum\"))\n",
    "loyalty_score_band.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################################################################################################################\n",
    "# Saving Loyalty Indicator to GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-02-20 15:26:09.306517\n"
     ]
    }
   ],
   "source": [
    "#Saving data into GCS\n",
    "(loyalty_score_band.coalesce(1).write.option(\"header\", \"true\").mode(\"overwrite\").csv(\"gs://westfield-tom/datalab/Loyalty_indicator/loyalty_score\"))\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reading data in from GCS\n",
    "loyalty_score_band = (spark.read.format(\"csv\")\n",
    "      .option(\"header\", \"true\").option(\"inferSchema\", \"true\")\n",
    "      .load('gs://westfield-tom/datalab/Loyalty_indicator/loyalty_score/part-00000-6f33c521-35de-4466-9b8e-aa55c4eb1a15-c000.csv'))\n",
    "\n",
    "loyalty_score_band.show(5)\n",
    "loyalty_score_band.count()\n",
    "\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################################################################################################################\n",
    "# Profile Loyalty Indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------------+\n",
      "|loyalty_tier|count(DISTINCT identifier)|\n",
      "+------------+--------------------------+\n",
      "|  1.Platinum|                    525533|\n",
      "|      2.Gold|                    503360|\n",
      "|    3.Silver|                   1097143|\n",
      "|    4.Bronze|                   2441740|\n",
      "|5.Occasional|                   7761392|\n",
      "+------------+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Customers per Tier\n",
    "cust_tier=loyalty_score_band.groupBy(col(\"loyalty_tier\")).agg(countDistinct(\"identifier\").alias(\"Customers\")).sort((col(\"loyalty_tier\")))\n",
    "cust_tier.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------+\n",
      "|loyalty_score_band|Customers|\n",
      "+------------------+---------+\n",
      "|                 0|  5762805|\n",
      "|                 1|  1878205|\n",
      "|                 2|  1711994|\n",
      "|                 3|   980069|\n",
      "|                 4|   737141|\n",
      "|                 5|   480063|\n",
      "|                 6|   327619|\n",
      "|                 7|   212452|\n",
      "|                 8|   136824|\n",
      "|                 9|    82141|\n",
      "|                10|    19855|\n",
      "+------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Customers per score distribution\n",
    "cust_score_band=loyalty_score_band.withColumn(\"loyalty_score_band\",floor(col('loyalty_score')*10)).groupBy('loyalty_score_band').agg(countDistinct(\"identifier\").alias(\"Customers\")).sort('loyalty_score_band')\n",
    "cust_score_band.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------+\n",
      "|loyalty_score_band|Customers|\n",
      "+------------------+---------+\n",
      "|                 0|  4462788|\n",
      "|                 1|   289121|\n",
      "|                 2|   230976|\n",
      "|                 3|   130184|\n",
      "|                 4|    88440|\n",
      "|                 5|    83570|\n",
      "|                 6|   123918|\n",
      "|                 7|   132756|\n",
      "|                 8|   106047|\n",
      "|                 9|   115005|\n",
      "|                10|   136276|\n",
      "|                11|   151284|\n",
      "|                12|   161012|\n",
      "|                13|   232747|\n",
      "|                14|   135956|\n",
      "|                15|   190129|\n",
      "|                16|   139698|\n",
      "|                17|   263948|\n",
      "|                18|   232760|\n",
      "|                19|   234395|\n",
      "|                20|   191952|\n",
      "|                21|   228246|\n",
      "|                22|   216414|\n",
      "|                23|   224505|\n",
      "|                24|   249729|\n",
      "|                25|   111822|\n",
      "|                26|   134453|\n",
      "|                27|   123671|\n",
      "|                28|   118895|\n",
      "|                29|   112307|\n",
      "|                30|   137805|\n",
      "|                31|   120403|\n",
      "|                32|   105656|\n",
      "|                33|   102365|\n",
      "|                34|    90791|\n",
      "|                35|    86794|\n",
      "|                36|    83528|\n",
      "|                37|    92680|\n",
      "|                38|    79965|\n",
      "|                39|    80082|\n",
      "|                40|    80681|\n",
      "|                41|    88705|\n",
      "|                42|    78968|\n",
      "|                43|    81263|\n",
      "|                44|    78052|\n",
      "|                45|    78445|\n",
      "|                46|    67503|\n",
      "|                47|    65096|\n",
      "|                48|    61438|\n",
      "|                49|    56990|\n",
      "|                50|    55462|\n",
      "|                51|    52028|\n",
      "|                52|    51203|\n",
      "|                53|    49958|\n",
      "|                54|    49203|\n",
      "|                55|    46628|\n",
      "|                56|    45665|\n",
      "|                57|    44975|\n",
      "|                58|    43410|\n",
      "|                59|    41531|\n",
      "|                60|    39500|\n",
      "|                61|    38721|\n",
      "|                62|    37090|\n",
      "|                63|    34455|\n",
      "|                64|    32899|\n",
      "|                65|    31363|\n",
      "|                66|    30081|\n",
      "|                67|    29308|\n",
      "|                68|    27800|\n",
      "|                69|    26402|\n",
      "|                70|    25348|\n",
      "|                71|    25057|\n",
      "|                72|    23654|\n",
      "|                73|    22370|\n",
      "|                74|    21787|\n",
      "|                75|    20739|\n",
      "|                76|    19476|\n",
      "|                77|    18659|\n",
      "|                78|    17896|\n",
      "|                79|    17466|\n",
      "|                80|    16205|\n",
      "|                81|    15746|\n",
      "|                82|    15317|\n",
      "|                83|    14641|\n",
      "|                84|    13845|\n",
      "|                85|    13242|\n",
      "|                86|    12881|\n",
      "|                87|    12198|\n",
      "|                88|    11599|\n",
      "|                89|    11150|\n",
      "|                90|    10664|\n",
      "|                91|    10485|\n",
      "|                92|     9642|\n",
      "|                93|     9126|\n",
      "|                94|     8708|\n",
      "|                95|     8005|\n",
      "|                96|     7275|\n",
      "|                97|     6643|\n",
      "|                98|     6042|\n",
      "|                99|     5551|\n",
      "|               100|    19855|\n",
      "+------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Customers per detailed score distribution\n",
    "cust_detail_score_band=loyalty_score_band.withColumn(\"loyalty_score_band\",floor(col('loyalty_score')*100)).groupBy('loyalty_score_band').agg(countDistinct(\"identifier\").alias(\"Customers\")).sort('loyalty_score_band')\n",
    "cust_detail_score_band.show(101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
